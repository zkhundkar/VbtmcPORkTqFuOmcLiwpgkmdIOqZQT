{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project - InstaCart Market Basket Analysis\n",
    "\n",
    "### Utility functions for pre-processing and feature engineering\n",
    "This notebook duplicates the functions I created for pre-processing and building new features for the project. \n",
    "I have included them as a separate notebook for easier viewing and better way to document the code\n",
    "\n",
    "The project uses pandas v0.18 and relies on saving dataframes in hdfs format (pandas native) for efficiency. This option requires the `tables` (pytables) module to be installed on the host machine. I needed to install this in my Anaconda2 environment using `conda`.\n",
    "\n",
    "The first block here is boilerplate code, used by other notebooks I created for downstream use. Importing gc helps with managing the memory footprint during the pre-processing and analysis stages."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print \"Loaded numpy and pandas libraries\"\n",
    "\n",
    "# The _datapath variable should point to the folder where the hdfs files are located\n",
    "_datapath = './'\n",
    "os.chdir(_datapath)\n",
    "\n",
    "# Named hdfs stores\n",
    "RAW_STORE = 'instacart_raw.hdf5' # direct copies of the csv files in hdfs format\n",
    "HDF_STORE = 'instacart.hdf5'     # stores the augmented products_priors dataframe which is used to produce the eng. features\n",
    "FVARS_STORE = 'features.hdf5'    # stores engineered features for use in analysis\n",
    "RANDOM_STATE = 46\n",
    "\n",
    "def get_from_hdf(tnames, STORE=RAW_STORE):\n",
    "    hnames = {}\n",
    "    result = {}\n",
    "    for n in tnames:\n",
    "        name = hname[n] if n in hnames else n\n",
    "        try:\n",
    "            print \"Loading {} datasets ...\".format(n)\n",
    "            result[n] = pd.read_hdf(STORE, name)\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            print \"Dataset could not be loaded. Is the hdf store missing?\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting orders and products\n",
    "\n",
    "- `get_sample` : is a utility function to enable us to take a small randomized sample of a large dataframe during development\n",
    "- `preprocess_orders` : handles missing values and outliers in the `orders` table. The `days_since_prior_order` uses a value of _30_ for true values greater than or equal to 30. We modified this to a random number between 30 and 50 and added a categorical variable to indicate that those values are assigned during preprocessing. _Additionally_, this routine also calculates the cumulative number of days between a user's orders.\n",
    "- `add_product_groups` : adds `department_id` and `aisle_id` columns to the order history (`priors`)\n",
    "- `get_hist_chunk` : another function to assist development and code testing. This takes a random sample (size determined by `frac`) of users and then selects their order history. This enables us to go through the pre-processing logic in small chunks to verify that the logic is working correctly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_sample(p, **kw):\n",
    "    r\"\"\"\n",
    "        requires two keyword inputs \n",
    "        by = column name\n",
    "        frac = fraction of rows to sample\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        colname = kw['by']\n",
    "        frac = kw['frac'] if frac in kw else 0.05\n",
    "        u = p[[colname]].unique()\n",
    "        u = u.sample(frac=frac, random_state=seed)\n",
    "        ochunk = p[p[[colname]].isin(u)]\n",
    "    except NameError as e:\n",
    "        ochunk = \"\"\n",
    "        print e\n",
    "    finally:\n",
    "        del u\n",
    "    print \"Selected {}={:.2f}% rows of {} {}.\".format(len(ochunk),\\\n",
    "            100.0*frac, int(len(u)*frac), colname)\n",
    "    return ochunk\n",
    "\n",
    "def preprocess_orders(orders):\n",
    "\n",
    "    orders.sort_values(by=['user_id', 'order_number'], inplace=True)\n",
    "\n",
    "    # Add additional columns\n",
    "    x = orders.days_since_prior_order\n",
    "    orders['30plus_days'] = (x==30).astype('int8')\n",
    "\n",
    "    # Impute 'missing' days_since_prior_order values\n",
    "    orders.days_since_prior_order.fillna(orders.days_since_prior_order.mean(), inplace=True)\n",
    "    ser=orders[x==30].index\n",
    "    orders.days_since_prior_order[ser] = pd.DataFrame(np.random.randint(30,50,len(ser)), index=ser)\n",
    "    orders['csum_ds'] = orders.groupby('user_id')['days_since_prior_order'].transform('cumsum') \n",
    "\n",
    "    # Transform\n",
    "    orders['log_ds'] = np.log(orders.days_since_prior_order+1)\n",
    "\n",
    "def add_product_groups(p, products):\n",
    "    return p.merge(products[['product_id', 'department_id', 'aisle_id']], on='product_id')\n",
    "\n",
    "def get_hist_chunk(orders, priors, frac=0.05, seed=RANDOM_STATE, **kw):\n",
    "    if 'start' in kw:\n",
    "        start = kw['start']\n",
    "        chunk = kw['size']\n",
    "        ochunk = orders.iloc[start:start+chunk]\n",
    "        print \"Selected {} orders {}:{}.\".format(len(ochunk), start, start+chunk)\n",
    "    elif 'all' in kw:\n",
    "        ochunk = orders\n",
    "        print \"Selected all orders.\"\n",
    "    else:\n",
    "        u = orders.groupby('user_id')['order_id'].last().reset_index()[['user_id']]\n",
    "        u = u.sample(frac=frac, random_state=seed)\n",
    "        ochunk = u.merge(orders, how=\"inner\", on='user_id')\n",
    "        print \"Selected {} orders for {} users.\".format(len(ochunk), len(u))\n",
    "    ochunk.sort_values(by=['user_id', 'order_number'], inplace=True)\n",
    "    p = ochunk.merge(priors, on='order_id')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one feature that required special handling. We are given the days between orders, but a product may not be on each order placed by a user. In order to calculate the average number of days between when a user orders a specific product, we took these steps:\n",
    " 1. sort the data by `user_id`, `product_id`, and `order_number`\n",
    " 2. take the difference of the cumulative sum of days since the last order (calculated in preprocess_orders and stored as `csum_ds`) of each row and its previous one. For the first row, we use the days since the previous order\n",
    " \n",
    " While there is a way to calculate this in `pandas` using a lambda function, the computation took a very long time. In the interest of efficiency, I wrote a function (`diff1`) to achieve this using `numpy` matrices. TI was able to achive a 20-fold speedup using this method (estimate based on small samples)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def diff_1(m):\n",
    "    r\"\"\" Takes a matrix where each row is strucured as [user_id, product_id, order_number and cumulative sum of\n",
    "        the number of days since previous order. \n",
    "    \"\"\"\n",
    "    user = product = \"\"\n",
    "    last = 0\n",
    "    for row in m:  \n",
    "        if (user == row[0]) and (product == row[1]): \n",
    "            # another order for the same user and product\n",
    "            save = row[3]  \n",
    "            row[3] -= last # diff between this row's cumsum and the previous row for this product and user\n",
    "            last = save\n",
    "        else:\n",
    "            # got a different use or product, so set the 'current' user and product\n",
    "            user = row[0]\n",
    "            product = row[1]\n",
    "            last = row[3] # set the last value for cumsum of this user/product combo\n",
    "\n",
    "    return m\n",
    "\n",
    "def user_product_ds_last(p):\n",
    "\n",
    "        p2 = p[['user_id', 'product_id', 'order_number', 'csum_ds']]\\\n",
    "                .sort_values(by=['user_id', 'product_id', 'order_number'])\n",
    "        g = pd.DataFrame(diff_1(p2.as_matrix()), dtype='int32')\n",
    "        g.columns = ['user_id', 'product_id', 'order_number', 'ds_last']\n",
    "        print len(g), g.columns \n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building user-product features\n",
    "\n",
    "Building features for user-product combinations using extensive use of Dataframe `aggregate` function. Descriptions of these features are provided in the Preprocessing notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def user_product_features(phist, verbosity=3):\n",
    "\n",
    "    tic = tic0 = time.time()\n",
    "    try: # inline comments\n",
    "        up_feat = phist.groupby(['user_id', 'aisle_id', 'product_id'])\\\n",
    "                .agg({'reordered':['count', 'mean', 'last'], 'ds_last':'mean',\\\n",
    "                    'order_number':'max', 'add_to_cart_order':'mean'})\\\n",
    "                .reset_index()\n",
    "        up_feat.columns=['user_id', 'aisle_id', 'product_id', 'up_cart_rank', 'up_avg_days',\\\n",
    "             'up_in_last', 'up_times', 'up_reord_prob', 'up_last_reord']\n",
    "        up_feat['up_in_last'] = up_feat['up_in_last'].astype('int8')\n",
    "        toc = time.time()\n",
    "        if verbosity > 2:\n",
    "            print \"Step 1 / 4 - computed user-product averages {:.3f} s\".format(toc - tic)\n",
    "            tic = toc\n",
    "\n",
    "        u_orders = phist.groupby(['user_id', 'order_id'])['product_id'].count().reset_index()\n",
    "        u_orders = u_orders.groupby('user_id').agg({'order_id':'nunique', 'product_id':'mean'})\\\n",
    "                        .reset_index()\n",
    "        u_orders.rename(columns={'order_id':'uo_count', 'product_id':'ubasket_avg'},\\\n",
    "                    inplace=True)\n",
    "        up_feat = up_feat.merge(u_orders, on='user_id')\n",
    "        up_feat[\"up_prob\"] = up_feat.up_times / up_feat.uo_count\n",
    "        del u_orders\n",
    "        toc = time.time()\n",
    "        if verbosity > 2:\n",
    "            print \"Step 2 / 4 - computed averages by order {:.3f} s\".format(toc-tic)\n",
    "            tic = toc\n",
    "\n",
    "        up_orders = phist.groupby(['user_id', 'product_id'])\\\n",
    "                    ['order_dow', 'order_hour_of_day', '30plus_days' ].mean()\\\n",
    "                    .reset_index()\n",
    "        up_orders.rename(columns={'order_dow':'up_dow', 'order_hour_of_day':'up_tod',\\\n",
    "                     '30plus_days':'up_30_avg'}, inplace=True)\n",
    "        up_feat = up_feat.merge(up_orders, on=['user_id', 'product_id'])\n",
    "        del up_orders\n",
    "\n",
    "        toc = time.time()\n",
    "        if verbosity > 2:\n",
    "            print \"Step 3 / 4 - computed average by product {:.3f} s\".format(toc-tic)\n",
    "            tic = toc\n",
    "\n",
    "        pa_up = phist.groupby(['user_id', 'order_id', 'aisle_id'])\\\n",
    "            .agg({'product_id':'nunique',\\\n",
    "                'reordered':'sum'}).reset_index()\n",
    "        pa_up.columns = ['user_id', 'order_id', 'aisle_id', 'u_a_pcount', 'u_a_reord']\n",
    "        pa_up = pa_up.groupby(['user_id', 'aisle_id'])\\\n",
    "                .agg({'order_id':'count', 'u_a_pcount':'mean', 'u_a_reord':'mean'})\\\n",
    "                .reset_index()\n",
    "        pa_up.rename(columns={'order_id':'ua_ocount'}, inplace=True)\n",
    "        print pa_up.columns\n",
    "        up_feat = up_feat.merge(pa_up, on=['user_id', 'aisle_id'])\n",
    "        toc = time.time()\n",
    "        if verbosity > 2:\n",
    "            print \"Step 4 / 4 {:.3f} s\".format(toc - tic)\n",
    "            tic = toc\n",
    "    except:\n",
    "        up_feat = None\n",
    "\n",
    "    toc = time.time()\n",
    "    if verbosity > -1:\n",
    "        print \"Completed in a total of {:.3f} s\".format(toc - tic0)\n",
    "        tic = toc\n",
    "        print up_feat.columns\n",
    "    return up_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building user's top-_N_ aisles feature\n",
    "\n",
    "The function below analyses users' puchase patterns across orders to identify which groups of items they tend to purchase most frequently. In this round, we are using aisles as a convenient group for products. The function counts the number of products per aisle, sorts them in descending order, then picks the aisle-ids for the top-N (N is an input) aisles. \n",
    "\n",
    "I rely on some matrix manipulation to convert the top-N list to a data frame with N-columns. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def build_user_topn_aisles(p, topN=10):\n",
    "\n",
    "    r\"\"\"\n",
    "        Create dataframe with the top-N aisles a user shops in\n",
    "        Input is a df from the priors table, with user_id and aisle_id\n",
    "    \"\"\"\n",
    "\n",
    "    print \"{} products ordered\".format(len(p))\n",
    "    gg = p.groupby(['user_id', 'order_id', 'aisle_id'])['product_id'].count().reset_index()\n",
    "    print \" ..  including products from {} aisles\".format(len(gg))\n",
    "    gg = gg.groupby(['user_id', 'aisle_id'])['product_id'].sum().reset_index()\\\n",
    "            .rename(columns={'product_id':'oa_count'})\n",
    "    gg = gg.sort_values(by=['user_id', 'oa_count'], ascending=False)[['user_id', 'aisle_id']]\n",
    "    gm = gg.groupby('user_id')['aisle_id'].unique().reset_index().as_matrix()\n",
    "    print \" .. {} users\".format(gm.shape[0])\n",
    "\n",
    "    dm = np.zeros((topN+1)*len(gm)).reshape(len(gm), topN+1)\n",
    "    for i in xrange(len(gm)):\n",
    "        dm[i] = np.concatenate([[gm[i][0]], gm[i][1], np.zeros(topN)])[:topN+1]\n",
    "    g = pd.DataFrame(dm, dtype=\"int32\")\n",
    "    g.columns=['user_id']+['top-{}'.format(x) for x in range(1,topN+1)]\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Models and prediction functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ---------- Benchmark 1 -- \n",
    "def bench1_fit(priors, orders):\n",
    "    pset = orders[orders.eval_set==0][['order_id', 'user_id']]\n",
    "    bench = priors.merge(pset,  on='order_id')\n",
    "\n",
    "    # Calculate product popularity\n",
    "    popular = bench[bench.reordered==1].groupby('product_id')['order_id'].count().reset_index()\\\n",
    "    .rename(columns={'order_id':'frequency'})\n",
    "    popular.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "\n",
    "    #Calculate average basket size\n",
    "    basket_size = bench.groupby('order_id')['product_id'].count()\\\n",
    "    .reset_index().rename(columns={'product_id':'Average Size'})\n",
    "    topN = int(round(np.float(basket_size[[1]].mean()[0]), 0))\n",
    "    print \"Average basket size is {:.3f}\".format(topN)\n",
    "    \n",
    "    model = popular.product_id[:topN].reset_index().drop('index', axis=1)\n",
    "    model['in_order'] = (np.ones(len(model))).astype('uint8')\n",
    "    return model\n",
    "\n",
    "# ---------- Benchmark 2 -- \n",
    "def bench2_fit(priors, orders):\n",
    "    # Add user_id to the historical set\n",
    "    pset = orders[orders.eval_set==0][['order_id', 'user_id', 'order_number']]\n",
    "    bench = priors.merge(pset,  on='order_id')\n",
    "    model = bench.groupby(['user_id', 'product_id'])['order_number'].last().reset_index()\n",
    "    model.rename(columns={'last':'in_order', 'order_number':'in_order'}, inplace=True)\n",
    "    model.in_order = (model.in_order / model.in_order).astype('uint8')\n",
    "\n",
    "    return model\n",
    "\n",
    "# ---------- Benchmark 3 -- \n",
    "def bench3_fit(priors, orders):\n",
    "    # Find the history set -- prior and train\n",
    "    bench = orders[orders.eval_set==0][['order_id', 'user_id', 'order_number']]\n",
    "    bench3 = bench.merge(priors, on='order_id')\n",
    "    # Calculate most frequently purchased items by user\n",
    "    popular = bench3.groupby(['user_id','product_id'])['order_id'].count().reset_index()\\\n",
    "                .rename(columns={'order_id':'frequency'})\n",
    "    popular['prod_rank']=popular.groupby(['user_id'])['frequency']\\\n",
    "                .rank(ascending=False).astype('uint16')\n",
    "\n",
    "    #Calculate average basket size\n",
    "    u_baskets = bench3.groupby(['user_id', 'order_id'])['product_id'].count()\\\n",
    "        .reset_index().rename(columns={'product_id':'basket_size'})\n",
    "    u_baskets = u_baskets.groupby('user_id')['basket_size'].mean().reset_index()\n",
    "    model = popular.merge(u_baskets, on='user_id') \n",
    "    model['in_order'] = model.prod_rank[model.prod_rank<model.basket_size]\n",
    "    return model[['user_id', 'product_id', 'in_order']]\n",
    "\n",
    "# ---------- Benchmark predictions -- \n",
    "\n",
    "def\tbench_predict(model, X):\n",
    "    # Input X should be a dataframe including a user_id\n",
    "    # Input model should be a dataframe with two or three columns:\n",
    "    #\t\t[user_id], product_id and in_order\n",
    "\n",
    "    if 'user_id' in model.columns:\n",
    "        X = X.merge(model[['user_id','product_id','in_order']], how='left', \n",
    "            on=['user_id', 'product_id'])\n",
    "    else:\n",
    "        X = X.merge(model, how='left', on='product_id')\n",
    "    pred = X.in_order.fillna(0)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
